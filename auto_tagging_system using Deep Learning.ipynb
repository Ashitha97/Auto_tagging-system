{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tables in c:\\users\\ashitha a nair\\.conda\\envs\\tensorflow\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.9.3 in c:\\users\\ashitha a nair\\.conda\\envs\\tensorflow\\lib\\site-packages (from tables) (1.19.1)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in c:\\users\\ashitha a nair\\.conda\\envs\\tensorflow\\lib\\site-packages (from tables) (2.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_questions = pd.read_hdf('auto_tagging_data_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learning?</td>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=\"http://anyall.org/\"&gt;Brendan O'Connor&lt;/a&gt; entitled &lt;a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\"&gt;\"Statistics vs. Mach...</td>\n",
       "      <td>[machine-learning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>Forecasting demographic census</td>\n",
       "      <td>&lt;p&gt;What are some of the ways to forecast demographic census with some validation and calibration techniques?&lt;/p&gt;\\n\\n&lt;p&gt;Some of the concerns:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Census blocks vary in sizes as rural\\n...</td>\n",
       "      <td>[forecasting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain English</td>\n",
       "      <td>&lt;p&gt;How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?&lt;/p&gt;\\n</td>\n",
       "      <td>[bayesian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>What is the meaning of p values and t values in statistical tests?</td>\n",
       "      <td>&lt;p&gt;After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....</td>\n",
       "      <td>[hypothesis-testing, t-test, p-value, interpretation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>Examples for teaching: Correlation does not mean causation</td>\n",
       "      <td>&lt;p&gt;There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;number of storks and birth ...</td>\n",
       "      <td>[correlation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                                               Title  \\\n",
       "0   6                  The Two Cultures: statistics vs. machine learning?   \n",
       "1  21                                      Forecasting demographic census   \n",
       "2  22                 Bayesian and frequentist reasoning in plain English   \n",
       "3  31  What is the meaning of p values and t values in statistical tests?   \n",
       "4  36          Examples for teaching: Correlation does not mean causation   \n",
       "\n",
       "                                                                                                                                                                                                      Body  \\\n",
       "0  <p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\">\"Statistics vs. Mach...   \n",
       "1  <p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census blocks vary in sizes as rural\\n...   \n",
       "2                                                                               <p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>\\n   \n",
       "3  <p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....   \n",
       "4  <p>There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:</p>\\n\\n<ol>\\n<li>number of storks and birth ...   \n",
       "\n",
       "                                                    Tags  \n",
       "0                                     [machine-learning]  \n",
       "1                                          [forecasting]  \n",
       "2                                             [bayesian]  \n",
       "3  [hypothesis-testing, t-test, p-value, interpretation]  \n",
       "4                                          [correlation]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine title and body\n",
    "df_questions['Text'] = df_questions[\"Title\"] + \" \" + df_questions[\"Body\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove html tags and url links\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove everything alphabets\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "    # remove whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions['Text'] = df_questions['Text'].apply(lambda x: clean_text(x))\n",
    "df_questions['Text'] = df_questions['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49539</th>\n",
       "      <td>95407</td>\n",
       "      <td>how to create a dependent variable in regression when it does not exist in data in ques i have factors which could affect purchasing decision now i wanted to find the co relation between factors w...</td>\n",
       "      <td>[regression, correlation, chi-squared]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50515</th>\n",
       "      <td>79311</td>\n",
       "      <td>repeated measures glm restricted permutations i have a question regarding repeated measures and glms suppose i have counted the abundance of some species in lakes at different time points every la...</td>\n",
       "      <td>[r, repeated-measures, generalized-linear-model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39436</th>\n",
       "      <td>37832</td>\n",
       "      <td>decomposition of the sum of two random variables we observe the input samples as z x y where distribution of z could be estimated by histogram of samples and x y are two independent random variabl...</td>\n",
       "      <td>[probability]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8003</th>\n",
       "      <td>26951</td>\n",
       "      <td>given a two populations each split by categories how do i test the hypothesis that the mean is larger in the second population the means for each category varies but my hypothesis is that the seco...</td>\n",
       "      <td>[hypothesis-testing, anova, mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64076</th>\n",
       "      <td>77270</td>\n",
       "      <td>feature selection and cross validation i m working on a project and i would like to know if the following strategy is good correct sorry if this is a basic stupid idea i m new to this the input is...</td>\n",
       "      <td>[machine-learning, data-mining, svm, cross-validation, feature-selection]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  \\\n",
       "49539  95407   \n",
       "50515  79311   \n",
       "39436  37832   \n",
       "8003   26951   \n",
       "64076  77270   \n",
       "\n",
       "                                                                                                                                                                                                          Text  \\\n",
       "49539  how to create a dependent variable in regression when it does not exist in data in ques i have factors which could affect purchasing decision now i wanted to find the co relation between factors w...   \n",
       "50515  repeated measures glm restricted permutations i have a question regarding repeated measures and glms suppose i have counted the abundance of some species in lakes at different time points every la...   \n",
       "39436  decomposition of the sum of two random variables we observe the input samples as z x y where distribution of z could be estimated by histogram of samples and x y are two independent random variabl...   \n",
       "8003   given a two populations each split by categories how do i test the hypothesis that the mean is larger in the second population the means for each category varies but my hypothesis is that the seco...   \n",
       "64076  feature selection and cross validation i m working on a project and i would like to know if the following strategy is good correct sorry if this is a basic stupid idea i m new to this the input is...   \n",
       "\n",
       "                                                                            Tags  \n",
       "49539                                     [regression, correlation, chi-squared]  \n",
       "50515                           [r, repeated-measures, generalized-linear-model]  \n",
       "39436                                                              [probability]  \n",
       "8003                                           [hypothesis-testing, anova, mean]  \n",
       "64076  [machine-learning, data-mining, svm, cross-validation, feature-selection]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions[['Id', 'Text', 'Tags']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Text to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_questions['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81956"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique words count\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81957"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique words count\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df_questions['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the two cultures statistics vs machine learning last year i read a blog post from brendan o connor entitled statistics vs machine learning fight that discussed some of the differences between the two fields andrew gelman responded favorably to this simon blomberg from r s fortunes package to paraphrase provocatively machine learning is statistics minus any checking of models and assumptions brian d ripley about the difference between machine learning and statistics user vienna may season s greetings andrew gelman in that case maybe we should get rid of checking of models and assumptions more often then maybe we d be able to solve some of the problems that the machine learning people can solve but we can t there was also the statistical modeling the two cultures paper by leo breiman in which argued that statisticians rely too heavily on data modeling and that machine learning techniques are making progress by instead relying on the predictive accuracy of models has the statistics field changed over the last decade in response to these critiques do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines \n",
      "\n",
      "[1, 59, 9730, 255, 315, 482, 256, 577, 277, 2, 333, 3, 2733, 374, 36, 24159, 665, 20232, 9413, 255, 315, 482, 256, 12412, 11, 2734, 60, 5, 1, 441, 74, 1, 59, 2265, 3603, 3384, 5009, 18773, 4, 12, 6933, 50267, 36, 45, 34, 38034, 250, 4, 14051, 50268, 482, 256, 6, 255, 2548, 68, 1771, 5, 132, 7, 691, 9140, 70, 9568, 89, 1, 167, 74, 482, 256, 7, 255, 600, 21877, 264, 1580, 34, 10105, 3603, 3384, 8, 11, 143, 580, 54, 83, 103, 3655, 5, 1771, 5, 132, 7, 691, 92, 726, 81, 580, 54, 70, 19, 375, 4, 588, 60, 5, 1, 597, 11, 1, 482, 256, 320, 28, 588, 31, 54, 28, 17, 43, 97, 105, 1, 235, 772, 1, 59, 9730, 433, 55, 16689, 7210, 8, 47, 6113, 11, 3184, 3258, 427, 3024, 21, 13, 772, 7, 11, 482, 256, 865, 18, 912, 3570, 55, 424, 6334, 21, 1, 785, 406, 5, 132, 100, 1, 255, 1056, 1670, 186, 1, 577, 8085, 8, 234, 4, 94, 15913, 48, 1, 59, 9730, 330, 1223, 33, 100, 255, 6392, 4, 16690, 482, 256, 865, 164, 25, 478, 886, 7, 855, 270, 2100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "print(df_questions['Text'][i], '\\n'), print(sequences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find right maximum length for the sequences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = []\n",
    "\n",
    "for i in sequences:\n",
    "    seq_lengths.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30th percentile:  97.0\n",
      "40th percentile:  116.0\n",
      "50th percentile:  137.0\n",
      "60th percentile:  162.0\n",
      "70th percentile:  193.0\n",
      "80th percentile:  238.0\n",
      "90th percentile:  320.0\n",
      "95th percentile:  411.0\n",
      "99th percentile:  678.0\n"
     ]
    }
   ],
   "source": [
    "print(\"30th percentile: \", pd.Series(seq_lengths).quantile(0.3))\n",
    "print(\"40th percentile: \", pd.Series(seq_lengths).quantile(0.4))\n",
    "print(\"50th percentile: \", pd.Series(seq_lengths).quantile(0.5))\n",
    "print(\"60th percentile: \", pd.Series(seq_lengths).quantile(0.6))\n",
    "print(\"70th percentile: \", pd.Series(seq_lengths).quantile(0.7))\n",
    "print(\"80th percentile: \", pd.Series(seq_lengths).quantile(0.8))\n",
    "print(\"90th percentile: \", pd.Series(seq_lengths).quantile(0.9))\n",
    "print(\"95th percentile: \", pd.Series(seq_lengths).quantile(0.95))\n",
    "print(\"99th percentile: \", pd.Series(seq_lengths).quantile(0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 125\n",
    "\n",
    "# padding\n",
    "padded_seq = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(df_questions['Tags'])\n",
    "y = multilabel_binarizer.transform(df_questions['Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76365, 125), (76365, 100))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(padded_seq, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(2000, 128, input_length = max_length))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Conv1D(300, 3, padding = 'valid', activation = \"relu\", strides = 1))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(100, activation = \"sigmoid\"))\n",
    "#model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 125, 128)          256000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 123, 300)          115500    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "=================================================================\n",
      "Total params: 401,600\n",
      "Trainable params: 401,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "             EarlyStopping(patience=3),\n",
    "             ModelCheckpoint(filepath='model-conv1d_v1.h5', save_best_only=True)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=512,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Performane Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the code below to load the saved model\n",
    "# model = load_model('model-conv1d_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set threshold to 0.45\n",
    "preds_int = (preds >= 0.45).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# calculate F1 score\n",
    "f1_score(y_val, preds_int, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('model-conv1d_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tags(q):\n",
    "    q = clean_text(q)\n",
    "    q = q.lower()\n",
    "    q_seq = tokenizer.texts_to_sequences([q])\n",
    "    q_seq_padded = pad_sequences(q_seq, maxlen=300)\n",
    "    q_pred = model.predict(q_seq_padded)\n",
    "    q_pred = (q_pred >= 0.3).astype(int)\n",
    "    \n",
    "    return multilabel_binarizer.inverse_transform(q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give new question\n",
    "new_q = \"Regression line in ggplot doesn't match computed regression Im using R and created a chart using ggplot2. I then create a regression so I can make some predicitions I pass my data frame of to the predict function predict(regression, Measures) I'd expect the predictions to be the same as if I used the regression line on the chart, but they aren't the same. Why would this be the case? Is there a setting in ggplot or is my expectation incorrect?\"\n",
    "\n",
    "# get tags\n",
    "infer_tags(new_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
